{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **HW2 : Decision Tree and Random Forest**\n","In *assignment 2*, you need to finish :\n","\n","1. Basic Part : Implement a **Decision Tree** model and predict whether the patients in the validation set have diabetes\n","> * Step 1 : Load the input data\n","> * Step 2 : Calculate the Entropy and Information Gain\n","> * Step 3 : Find the Best Split\n","> * Step 4 : Split into 2 branches\n","> * Step 5 : Build decision tree\n","> * Step 6 : Save the answers from step2 to step5\n","> * Step 7 : Split data into training set and validation set\n","> * Step 8 : Train a decision tree model with training set\n","> * Step 9 : Predict the cases in the *validation set* by using the model trained in *Step8*\n","> * Step 10 : Calculate the f1-score of your predictions in *Step9*\n","> * Step 11 : Write the Output File\n","\n","2. Advanced Part : Build a **Random Forest** model to make predictions\n","> * Step 1 : Load the input data\n","> * Step 2 : Load the test data\n","> * Step 3 : Build a random forest\n","> * Step 4 : Predict the cases in the test data by using the model trained in *Step3*\n","> * Step 5 : Save the predictions(from *Step 4*) in a csv file\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wwVh8lYD4kbV"},"source":["# **Basic Part** (60%)\n","In this part, your need to implement a Decision Tree model by completing the following given functions.\n","\n","Also, you need to run these functions with the given input variables and save the output in a csv file **hw2_basic.csv**"]},{"cell_type":"markdown","metadata":{"id":"h2ibEyDa46X2"},"source":["## Import Packages\n","\n","\n","> Note : You **cannot** import any other packages in both basic part and advanced part\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"RMjaYVZD6kmb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","import random\n","from numpy import sqrt\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"zrQXqH475G8-"},"source":["## Step1: Load the input data\n","First, load the input file **hw2_input_basic.csv**"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"0n3gcL2l6kjb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>bmi</th>\n","      <th>gender</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>glucose_apache</th>\n","      <th>heart_rate_apache</th>\n","      <th>resprate_apache</th>\n","      <th>sodium_apache</th>\n","      <th>diabetes_mellitus</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>70.0</td>\n","      <td>25.984659</td>\n","      <td>1</td>\n","      <td>172.7</td>\n","      <td>77.50</td>\n","      <td>116.0</td>\n","      <td>101.0</td>\n","      <td>49.0</td>\n","      <td>137.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30.0</td>\n","      <td>31.310368</td>\n","      <td>1</td>\n","      <td>170.2</td>\n","      <td>90.70</td>\n","      <td>71.0</td>\n","      <td>39.0</td>\n","      <td>33.0</td>\n","      <td>144.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>54.0</td>\n","      <td>24.388824</td>\n","      <td>1</td>\n","      <td>177.8</td>\n","      <td>77.10</td>\n","      <td>120.0</td>\n","      <td>120.0</td>\n","      <td>31.0</td>\n","      <td>141.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>65.0</td>\n","      <td>34.141074</td>\n","      <td>0</td>\n","      <td>170.2</td>\n","      <td>98.90</td>\n","      <td>73.0</td>\n","      <td>48.0</td>\n","      <td>36.0</td>\n","      <td>140.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>49.0</td>\n","      <td>22.564743</td>\n","      <td>1</td>\n","      <td>172.7</td>\n","      <td>67.30</td>\n","      <td>207.0</td>\n","      <td>119.0</td>\n","      <td>6.0</td>\n","      <td>144.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>62.0</td>\n","      <td>29.424010</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>70.60</td>\n","      <td>113.0</td>\n","      <td>60.0</td>\n","      <td>32.0</td>\n","      <td>137.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>85.0</td>\n","      <td>27.673574</td>\n","      <td>1</td>\n","      <td>154.9</td>\n","      <td>66.40</td>\n","      <td>102.0</td>\n","      <td>49.0</td>\n","      <td>36.0</td>\n","      <td>142.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>65.0</td>\n","      <td>22.269432</td>\n","      <td>1</td>\n","      <td>177.8</td>\n","      <td>70.40</td>\n","      <td>333.0</td>\n","      <td>59.0</td>\n","      <td>6.0</td>\n","      <td>145.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>85.0</td>\n","      <td>35.879362</td>\n","      <td>0</td>\n","      <td>165.1</td>\n","      <td>97.80</td>\n","      <td>124.0</td>\n","      <td>92.0</td>\n","      <td>30.0</td>\n","      <td>136.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>81.0</td>\n","      <td>20.859375</td>\n","      <td>0</td>\n","      <td>160.0</td>\n","      <td>53.40</td>\n","      <td>136.0</td>\n","      <td>118.0</td>\n","      <td>52.0</td>\n","      <td>138.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>59.0</td>\n","      <td>46.409136</td>\n","      <td>0</td>\n","      <td>162.6</td>\n","      <td>122.70</td>\n","      <td>169.0</td>\n","      <td>100.0</td>\n","      <td>46.0</td>\n","      <td>138.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>77.0</td>\n","      <td>32.324734</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>77.56</td>\n","      <td>264.0</td>\n","      <td>90.0</td>\n","      <td>37.0</td>\n","      <td>141.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>68.0</td>\n","      <td>15.913579</td>\n","      <td>1</td>\n","      <td>185.4</td>\n","      <td>54.70</td>\n","      <td>39.0</td>\n","      <td>108.0</td>\n","      <td>45.0</td>\n","      <td>135.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>51.0</td>\n","      <td>24.028492</td>\n","      <td>1</td>\n","      <td>190.5</td>\n","      <td>87.20</td>\n","      <td>80.0</td>\n","      <td>61.0</td>\n","      <td>30.0</td>\n","      <td>139.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>76.0</td>\n","      <td>34.216873</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>82.10</td>\n","      <td>306.0</td>\n","      <td>112.0</td>\n","      <td>40.0</td>\n","      <td>130.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>48.0</td>\n","      <td>26.516476</td>\n","      <td>1</td>\n","      <td>180.3</td>\n","      <td>86.20</td>\n","      <td>96.0</td>\n","      <td>133.0</td>\n","      <td>31.0</td>\n","      <td>137.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>82.0</td>\n","      <td>18.921389</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>45.40</td>\n","      <td>164.0</td>\n","      <td>103.0</td>\n","      <td>48.0</td>\n","      <td>134.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>78.0</td>\n","      <td>36.668167</td>\n","      <td>0</td>\n","      <td>167.6</td>\n","      <td>103.00</td>\n","      <td>282.0</td>\n","      <td>104.0</td>\n","      <td>42.0</td>\n","      <td>138.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>62.0</td>\n","      <td>19.108088</td>\n","      <td>1</td>\n","      <td>157.5</td>\n","      <td>47.40</td>\n","      <td>275.0</td>\n","      <td>108.0</td>\n","      <td>38.0</td>\n","      <td>131.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>73.0</td>\n","      <td>22.851562</td>\n","      <td>0</td>\n","      <td>160.0</td>\n","      <td>58.50</td>\n","      <td>178.0</td>\n","      <td>154.0</td>\n","      <td>55.0</td>\n","      <td>138.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>73.0</td>\n","      <td>20.351971</td>\n","      <td>0</td>\n","      <td>167.5</td>\n","      <td>57.10</td>\n","      <td>159.0</td>\n","      <td>110.0</td>\n","      <td>27.0</td>\n","      <td>139.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>59.0</td>\n","      <td>27.109238</td>\n","      <td>1</td>\n","      <td>177.8</td>\n","      <td>85.70</td>\n","      <td>85.0</td>\n","      <td>92.0</td>\n","      <td>6.0</td>\n","      <td>143.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>51.0</td>\n","      <td>37.954367</td>\n","      <td>0</td>\n","      <td>172.7</td>\n","      <td>113.20</td>\n","      <td>168.0</td>\n","      <td>60.0</td>\n","      <td>43.0</td>\n","      <td>119.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>80.0</td>\n","      <td>36.267895</td>\n","      <td>1</td>\n","      <td>180.3</td>\n","      <td>117.90</td>\n","      <td>138.0</td>\n","      <td>178.0</td>\n","      <td>43.0</td>\n","      <td>140.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>74.0</td>\n","      <td>25.856081</td>\n","      <td>1</td>\n","      <td>170.2</td>\n","      <td>74.90</td>\n","      <td>145.0</td>\n","      <td>40.0</td>\n","      <td>16.0</td>\n","      <td>146.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>61.0</td>\n","      <td>29.161972</td>\n","      <td>1</td>\n","      <td>180.3</td>\n","      <td>94.80</td>\n","      <td>267.0</td>\n","      <td>62.0</td>\n","      <td>58.0</td>\n","      <td>134.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>74.0</td>\n","      <td>38.111136</td>\n","      <td>1</td>\n","      <td>188.0</td>\n","      <td>134.70</td>\n","      <td>279.0</td>\n","      <td>112.0</td>\n","      <td>43.0</td>\n","      <td>132.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>65.0</td>\n","      <td>53.414791</td>\n","      <td>0</td>\n","      <td>166.4</td>\n","      <td>147.90</td>\n","      <td>121.0</td>\n","      <td>88.0</td>\n","      <td>32.0</td>\n","      <td>142.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>38.0</td>\n","      <td>33.438787</td>\n","      <td>0</td>\n","      <td>161.3</td>\n","      <td>87.00</td>\n","      <td>135.0</td>\n","      <td>46.0</td>\n","      <td>28.0</td>\n","      <td>138.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>82.0</td>\n","      <td>32.263238</td>\n","      <td>0</td>\n","      <td>162.6</td>\n","      <td>85.30</td>\n","      <td>111.0</td>\n","      <td>114.0</td>\n","      <td>22.0</td>\n","      <td>143.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     age        bmi  gender  height  weight  glucose_apache  \\\n","0   70.0  25.984659       1   172.7   77.50           116.0   \n","1   30.0  31.310368       1   170.2   90.70            71.0   \n","2   54.0  24.388824       1   177.8   77.10           120.0   \n","3   65.0  34.141074       0   170.2   98.90            73.0   \n","4   49.0  22.564743       1   172.7   67.30           207.0   \n","5   62.0  29.424010       0   154.9   70.60           113.0   \n","6   85.0  27.673574       1   154.9   66.40           102.0   \n","7   65.0  22.269432       1   177.8   70.40           333.0   \n","8   85.0  35.879362       0   165.1   97.80           124.0   \n","9   81.0  20.859375       0   160.0   53.40           136.0   \n","10  59.0  46.409136       0   162.6  122.70           169.0   \n","11  77.0  32.324734       0   154.9   77.56           264.0   \n","12  68.0  15.913579       1   185.4   54.70            39.0   \n","13  51.0  24.028492       1   190.5   87.20            80.0   \n","14  76.0  34.216873       0   154.9   82.10           306.0   \n","15  48.0  26.516476       1   180.3   86.20            96.0   \n","16  82.0  18.921389       0   154.9   45.40           164.0   \n","17  78.0  36.668167       0   167.6  103.00           282.0   \n","18  62.0  19.108088       1   157.5   47.40           275.0   \n","19  73.0  22.851562       0   160.0   58.50           178.0   \n","20  73.0  20.351971       0   167.5   57.10           159.0   \n","21  59.0  27.109238       1   177.8   85.70            85.0   \n","22  51.0  37.954367       0   172.7  113.20           168.0   \n","23  80.0  36.267895       1   180.3  117.90           138.0   \n","24  74.0  25.856081       1   170.2   74.90           145.0   \n","25  61.0  29.161972       1   180.3   94.80           267.0   \n","26  74.0  38.111136       1   188.0  134.70           279.0   \n","27  65.0  53.414791       0   166.4  147.90           121.0   \n","28  38.0  33.438787       0   161.3   87.00           135.0   \n","29  82.0  32.263238       0   162.6   85.30           111.0   \n","\n","    heart_rate_apache  resprate_apache  sodium_apache  diabetes_mellitus  \n","0               101.0             49.0          137.0                  0  \n","1                39.0             33.0          144.0                  0  \n","2               120.0             31.0          141.0                  0  \n","3                48.0             36.0          140.0                  1  \n","4               119.0              6.0          144.0                  0  \n","5                60.0             32.0          137.0                  0  \n","6                49.0             36.0          142.0                  0  \n","7                59.0              6.0          145.0                  1  \n","8                92.0             30.0          136.0                  0  \n","9               118.0             52.0          138.0                  0  \n","10              100.0             46.0          138.0                  0  \n","11               90.0             37.0          141.0                  1  \n","12              108.0             45.0          135.0                  0  \n","13               61.0             30.0          139.0                  0  \n","14              112.0             40.0          130.0                  1  \n","15              133.0             31.0          137.0                  0  \n","16              103.0             48.0          134.0                  0  \n","17              104.0             42.0          138.0                  1  \n","18              108.0             38.0          131.0                  1  \n","19              154.0             55.0          138.0                  1  \n","20              110.0             27.0          139.0                  0  \n","21               92.0              6.0          143.0                  0  \n","22               60.0             43.0          119.0                  1  \n","23              178.0             43.0          140.0                  1  \n","24               40.0             16.0          146.0                  1  \n","25               62.0             58.0          134.0                  1  \n","26              112.0             43.0          132.0                  1  \n","27               88.0             32.0          142.0                  0  \n","28               46.0             28.0          138.0                  0  \n","29              114.0             22.0          143.0                  1  "]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["input_data = pd.read_csv('hw2_input_basic.csv')\n","input_data"]},{"cell_type":"markdown","metadata":{"id":"BhtqUTG9Nlyz"},"source":["## Global attributes\n","Define the global attributes\n","> Note : You **cannot** modify the values of these attributes we given in the basic part"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"etfPC94oN_TO"},"outputs":[],"source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = input_data.shape[1] - 1"]},{"cell_type":"markdown","metadata":{"id":"V1FN1Z-tOFOo"},"source":["> You can add your own global attributes here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQ-OYop8ONnv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Gey7t_Yx5YML"},"source":["## Step2 : Calculate the Entropy and Information Gain \n","Calculate the information gain and entropy values before separate data into left subtree and right subtree"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"hpdNz3ij6keH"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_entropy =  0.987\n"]}],"source":["from math import log2\n","\n","def entropy(data):\n","  \"\"\"\n","  This function measures the amount of uncertainty in a probability distribution\n","  args: \n","  * data(type: DataFrame): the data you're calculating for the entropy\n","  return:\n","  * entropy_value(type: float): the data's entropy\n","  \"\"\"\n","\n","  total = data.shape[0]\n","  pos_case = data['diabetes_mellitus'].sum()\n","  p = pos_case/total\n","  e = -p*log2(p)-(1-p)*log2(1-p)\n","  entropy_value = round(e,5)\n","  \n","  return entropy_value\n","\n","# [Note] You have to save the value of \"ans_entropy\" into the output file\n","ans_entropy = entropy(input_data)\n","print(\"ans_entropy = \", ans_entropy)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"zCC_SiU26kbX"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_informationGain =  0.08296666666666663\n"]}],"source":["def information_gain(data, mask):\n","  \"\"\"\n","  This function will calculate the information gain\n","  args:\n","  * data(type: DataFrame): the data you're calculating for the information gain\n","  * mask(type: Series): partition information(left/right) of current input data, \n","    - boolean 1(True) represents split to left subtree\n","    - boolean 0(False) represents split to right subtree\n","  return:\n","  * ig(type: float): the information gain you can obtain by classify data with this given mask\n","  \"\"\"\n","  #Split data base on mask\n","  left_tree_true = data[mask]\n","  right_tree_false = data[~mask]\n","\n","  #Calculate the number of cases in left and right subtree respectively\n","  n_left_tree = left_tree_true.shape[0]\n","  n_right_tree = right_tree_false.shape[0]\n","  n_total = data.shape[0]\n","\n","  #Calculate the weight of left and right subtree \n","  weight_left = n_left_tree/n_total\n","  weight_right = n_right_tree/n_total\n","\n","  #Claculate the entropy of left and right subtrees\n","  left_entropy = entropy(left_tree_true)\n","  right_entropy = entropy(right_tree_false)\n","\n","  entropy_before = entropy(data)\n","  entropy_after = weight_left * left_entropy + weight_right * right_entropy\n","  \n","  ig = round(entropy_before - entropy_after,5)\n","\n","  return ig\n","\n","# [Note] You have to save the value of \"ans_informationGain\" into your output file\n","temp1 = np.zeros((int(input_data.shape[0]/4), 1), dtype=bool)\n","temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/4), 1)), dtype=bool)\n","temp_mask = np.concatenate((temp1, temp2))\n","df_mask = pd.DataFrame(temp_mask, columns=['mask'])\n","ans_informationGain = information_gain(input_data, df_mask['mask'])\n","print(\"ans_informationGain = \", ans_informationGain)"]},{"cell_type":"markdown","metadata":{"id":"9r8mrn7A55if"},"source":["## Step3 : Find the Best Split\n","Find the best split combination, **feature** and **threshold**, by calculating the information gain\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6gg7ig18XgM"},"outputs":[],"source":["def find_best_split(data):\n","  \"\"\"\n","  This function will find the best split combination of data\n","  args:\n","  * data(type: DataFrame): the input data\n","  return\n","  * best_ig(type: float): the best information gain you obtain\n","  * best_threshold(type: float): the value that splits data into 2 branches\n","  * best_feature(type: string): the feature that splits data into 2 branches\n","  \"\"\"\n","\n","  \n","  return best_ig, best_threshold, best_feature\n","\n","\n","# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n","ans_ig, ans_value, ans_name = find_best_split(input_data)\n","print(\"ans_ig = \", ans_ig)\n","print(\"ans_value = \", ans_value)\n","print(\"ans_name = \", ans_name)"]},{"cell_type":"markdown","metadata":{"id":"61hPUYvy6MTB"},"source":["## Step4 : Split into 2 branches\n","Using the best split combination you find in function *find_best_split()* to split data into Left Subtree and Right Subtree "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQRcjzCLCo4R"},"outputs":[],"source":["def make_partition(data, feature, threshold):\n","  \"\"\"\n","  This function will split the data into 2 branches\n","  args:\n","  * data(type: DataFrame): the input data\n","  * feature(type: string): the attribute(column name)\n","  * threshold(type: float): the threshold for splitting the data\n","  return:\n","  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n","  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n","  \"\"\"\n","\n","  \n","  return left, right\n","\n","\n","# [Note] You have to save the value of \"ans_left\" into the output file\n","left, right = make_partition(input_data, 'age', 61.0)\n","ans_left = left.shape[0]\n","print(\"ans_left = \", ans_left)"]},{"cell_type":"markdown","metadata":{"id":"GLzy6Yhg802x"},"source":["## Step5 : Build Decision Tree\n","Use the above functions to implement the decision tree\n","\n","Instructions: \n","1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n","2.  Use function *find_best_split()* to find the best split combination\n","3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n","4. Use function *make_partition()* to split the data into two parts\n","5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OAXVddKkvM2"},"outputs":[],"source":["def build_tree(data, max_depth, min_samples_split, depth):\n","  \"\"\"\n","  This function will build the decision tree\n","  args:\n","  * data(type: DataFrame): the data you want to apply to the decision tree\n","  * max_depth: the maximum depth of a decision tree\n","  * min_samples_split: the minimum number of instances required to do partition\n","  * depth: the height of the current decision tree\n","  return:\n","  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n","  \"\"\"\n","\n","  # check the condition of current depth and the remaining number of samples\n","  if ..... :\n","    # call find_best_split() to find the best combination\n","\n","    # check the value of information gain is greater than 0 or not \n","    if ..... :\n","      # update the depth\n","      \n","      # call make_partition() to split the data into two parts\n","\n","      # If there is no data split to the left tree OR no data split to the left tree\n","      if ..... :\n","        # return the label of the majority\n","        label = .....\n","        return label\n","      else:\n","        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n","        subtree = {question: []}\n","\n","        # call function build_tree() to recursively build the left subtree and right subtree\n","\n","        if left_subtree == right_subtree:\n","          subtree = left_subtree\n","        else:\n","          subtree[question].append(left_subtree)\n","          subtree[question].append(right_subtree)\n","    else:\n","      # return the label of the majority\n","      label = .....\n","      return label\n","  else:\n","    # return the label of the majority\n","    label = .....\n","    return label\n","\n","  return subtree"]},{"cell_type":"markdown","metadata":{"id":"qlIrw9Gu-M9-"},"source":["An example of the output from *build_tree()* \n","```\n","{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n","```\n","Therefore, \n","```\n","ans_features = ['bmi', 'age']\n","ans_thresholds = [33.5, 68.5]\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QW8wm1rD9dlS"},"outputs":[],"source":["ans_features = []\n","ans_thresholds = []\n","\n","decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n","decisionTree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_n0BfNSGejN"},"outputs":[],"source":["# [Note] You have to save the features in the \"decisionTree\" structure (from root to branch and leaf) into the output file\n","ans_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6H9zkN_GgK-"},"outputs":[],"source":["# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n","ans_thresholds"]},{"cell_type":"markdown","metadata":{"id":"rP0SU7tTweOX"},"source":["## Step6 : Save answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDO36kKEwh6C"},"outputs":[],"source":["basic = []\n","basic.append(ans_entropy)\n","basic.append(ans_informationGain)\n","basic.append(ans_ig)\n","basic.append(ans_value)\n","basic.append(ans_name)\n","basic.append(ans_left)\n","for i in range(len(ans_features)):\n","  basic.append(ans_features[i])\n","for m in range(len(ans_thresholds)):\n","  basic.append(ans_thresholds[m])"]},{"cell_type":"markdown","metadata":{"id":"7DotyrSZjYKi"},"source":["## Step7 : Split data\n","Split data into training set and validation set\n","> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjNM-n4i5mlG"},"outputs":[{"name":"stdout","output_type":"stream","text":["(30, 10)\n","(20, 10)\n","(10, 10)\n"]}],"source":["num_train = 20\n","num_validation = 10\n","\n","training_data = input_data.iloc[:num_train]\n","validation_data = input_data.iloc[-num_validation:]\n","\n","y_train = training_data[[\"diabetes_mellitus\"]]\n","x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = validation_data[[\"diabetes_mellitus\"]]\n","x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = y_validation.values.flatten()\n","\n","print(input_data.shape)\n","print(training_data.shape)\n","print(validation_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"GfKSt2gH74Uu"},"source":["## Step8 to Step10 : Make predictions with a decision tree"]},{"cell_type":"markdown","metadata":{"id":"BZqSVoJ48a3-"},"source":["Define the attributions of the decision tree\n","> You **cannot** modify the values of these attributes in this part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSlZ7FVB8eau"},"outputs":[],"source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = x_train.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"FrK-YqLmLH8p"},"source":["We have finished the function '*classify_data()*' below, however, you can modify this function if you prefer completing it on your own way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0piZ0blpFXVq"},"outputs":[],"source":["def classify_data(instance, tree):\n","  \"\"\"\n","  This function will predict/classify the input instance\n","  args:\n","  * instance: a instance(case) to be predicted\n","  return:\n","  * answer: the prediction result (the classification result)\n","  \"\"\"\n","  equation = list(tree.keys())[0] \n","  if equation.split()[1] == '<=':\n","    temp_feature = equation.split()[0]\n","    temp_threshold = equation.split()[2]\n","    if instance[temp_feature] > float(temp_threshold):\n","      answer = tree[equation][1]\n","    else:\n","      answer = tree[equation][0]\n","  else:\n","    if instance[equation.split()[0]] in (equation.split()[2]):\n","      answer = tree[equation][0]\n","    else:\n","      answer = tree[equation][1]\n","\n","  if not isinstance(answer, dict):\n","    return answer\n","  else:\n","    return classify_data(instance, answer)\n","\n","\n","def make_prediction(tree, data):\n","  \"\"\"\n","  This function will use your pre-trained decision tree to predict the labels of all instances in data\n","  args:\n","  * tree: the decision tree\n","  * data: the data to predict\n","  return:\n","  * y_prediction: the predictions\n","  \"\"\"\n","  \n","  # [Note] You can call the function classify_data() to predict the label of each instance\n","\n","\n","  return y_prediction\n","\n","\n","def calculate_score(y_true, y_pred):\n","  \"\"\"\n","  This function will calculate the f1-score of the predictions\n","  args:\n","  * y_true: the ground truth\n","  * y_pred: the predictions\n","  return:\n","  * score: the f1-score\n","  \"\"\"\n","\n","  \n","  return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IEu3z3s9TDu"},"outputs":[],"source":["decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n","\n","y_pred = make_prediction(decision_tree, x_validation)\n","\n","# [Note] You have to save the value of \"ans_f1score\" the your output file\n","ans_f1score = calculate_score(y_validation, y_pred)\n","print(\"ans_f1score = \", ans_f1score)"]},{"cell_type":"markdown","metadata":{"id":"IzzOKOwn-kod"},"source":["## Step11 : Write the Output File\n","Save all of your answers in a csv file, named as **hw2_basic.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0zsaWPL2qXn"},"outputs":[],"source":["ans_path = 'hw2_basic.csv'\n","\n","# [Note] You have to save the value of \"ans_f1score\" into the output file\n","basic.append(ans_f1score)\n","print(basic)\n","\n","pd.DataFrame(basic).to_csv(ans_path, header = None, index = None)"]},{"cell_type":"markdown","metadata":{"id":"tV25IjM7_aEn"},"source":["# **Advanced Part** (35%)"]},{"cell_type":"markdown","metadata":{"id":"knH1Ih0Pha7X"},"source":["## Step1: Load the input data\n","First, load the input file **hw2_input_advanced.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FthBdLxRhi9W"},"outputs":[],"source":["advanced_data = pd.read_csv('hw2_input_advanced.csv')"]},{"cell_type":"markdown","metadata":{"id":"vqLH49oBndRh"},"source":["You can split *advanced_data* into training set and validaiton set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9l0hLPVjncam"},"outputs":[],"source":["training_data = \n","validation_data = "]},{"cell_type":"markdown","metadata":{"id":"tFgbUY_ajVOK"},"source":["## Step2 : Load the test data\n","Load the input file **hw2_input_test.csv** to make predictions with the pre-trained random forest model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hW542KWNxVF"},"outputs":[],"source":["x_test = pd.read_csv('hw2_input_test.csv')\n","x_test"]},{"cell_type":"markdown","metadata":{"id":"mH-0DxyR9qWn"},"source":["## Step3 : Build a Random Forest"]},{"cell_type":"markdown","metadata":{"id":"8xbLxFW597FG"},"source":["Define the attributions of the random forest\n","> * You **can** modify the values of these attributes in advanced part\n","> * Each tree can have different attribute values\n","> * There must be **at least** 3 decision trees in the random forest model\n","> * Must use function *build_tree()* to build a random forest model\n","> * These are the parameters you can adjust : \n","\n","\n","    ```\n","    max_depth = \n","    depth = 0\n","    min_samples_split = \n","    \n","    # total number of trees in a random forest\n","    n_trees = \n","\n","    # number of features to train a decision tree\n","    n_features = \n","\n","    # the ratio to select the number of instances\n","    sample_size = \n","    n_samples = int(training_data.shape[0] * sample_size)\n","    ```\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LD8ndJ8ymzG3"},"outputs":[],"source":["# Define the attributes\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVl66f1aU36-"},"outputs":[],"source":["def build_forest(data, n_trees, n_features, n_samples):\n","  \"\"\"\n","  This function will build a random forest.\n","  args:\n","  * data: all data that can be used to train a random forest\n","  * n_trees: total number of tree\n","  * n_features: number of features\n","  * n_samples: number of instances\n","  return:\n","  * forest: a random forest with 'n_trees' of decision tree\n","  \"\"\"\n","\n","  # must reuse function build_tree()\n","  tree = build_tree(.....)\n","\n","  return forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zylo6C51m3OJ"},"outputs":[],"source":["forest = build_forest(training_data, n_trees, n_features, n_samples)"]},{"cell_type":"markdown","metadata":{"id":"dZb6EEYnnO05"},"source":["## Step4 : Make predictions with the random forest\n","> Note: Please print the f1-score of the predictions of each decision tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbHMZnMDnWpG"},"outputs":[],"source":["def make_prediction_forest(forest, data):\n","  \"\"\"\n","  This function will use the pre-trained random forest to make the predictions\n","  args:\n","  * forest: the random forest\n","  * data: the data used to predict\n","  return:\n","  * y_prediction: the predicted results\n","  \"\"\"\n","\n","\n","  return y_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hcd70ubwgHq4"},"outputs":[],"source":["y_pred_test = make_prediction_forest(forest, x_test)"]},{"cell_type":"markdown","metadata":{"id":"2ufa5bP9HveO"},"source":["## Step5 : Write the Output File\n","Save your predictions from the **random forest** in a csv file, named as **hw2_advanced.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdAQcE41JJYB"},"outputs":[],"source":["advanced = []\n","for i in range(len(y_pred_test)):\n","  advanced.append(y_pred_test[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pq121klSHwWO"},"outputs":[],"source":["advanced_path = 'hw2_advanced.csv'\n","pd.DataFrame(advanced).to_csv(advanced_path, header = None, index = None)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"}}},"nbformat":4,"nbformat_minor":0}
