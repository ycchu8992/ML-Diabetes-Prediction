{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **HW2 : Decision Tree and Random Forest**\n","In *assignment 2*, you need to finish :\n","\n","1. Basic Part : Implement a **Decision Tree** model and predict whether the patients in the validation set have diabetes\n","> * Step 1 : Load the input data\n","> * Step 2 : Calculate the Entropy and Information Gain\n","> * Step 3 : Find the Best Split\n","> * Step 4 : Split into 2 branches\n","> * Step 5 : Build decision tree\n","> * Step 6 : Save the answers from step2 to step5\n","> * Step 7 : Split data into training set and validation set\n","> * Step 8 : Train a decision tree model with training set\n","> * Step 9 : Predict the cases in the *validation set* by using the model trained in *Step8*\n","> * Step 10 : Calculate the f1-score of your predictions in *Step9*\n","> * Step 11 : Write the Output File\n","\n","2. Advanced Part : Build a **Random Forest** model to make predictions\n","> * Step 1 : Load the input data\n","> * Step 2 : Load the test data\n","> * Step 3 : Build a random forest\n","> * Step 4 : Predict the cases in the test data by using the model trained in *Step3*\n","> * Step 5 : Save the predictions(from *Step 4*) in a csv file\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wwVh8lYD4kbV"},"source":["# **Basic Part** (60%)\n","In this part, your need to implement a Decision Tree model by completing the following given functions.\n","\n","Also, you need to run these functions with the given input variables and save the output in a csv file **hw2_basic.csv**"]},{"cell_type":"markdown","metadata":{"id":"h2ibEyDa46X2"},"source":["## Import Packages\n","\n","\n","> Note : You **cannot** import any other packages in both basic part and advanced part\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"RMjaYVZD6kmb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","import random\n","from numpy import sqrt\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"zrQXqH475G8-"},"source":["## Step1: Load the input data\n","First, load the input file **hw2_input_basic.csv**"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"0n3gcL2l6kjb"},"outputs":[],"source":["input_data = pd.read_csv('hw2_input_basic.csv')"]},{"cell_type":"markdown","metadata":{"id":"BhtqUTG9Nlyz"},"source":["## Global attributes\n","Define the global attributes\n","> Note : You **cannot** modify the values of these attributes we given in the basic part"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"etfPC94oN_TO"},"outputs":[],"source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = input_data.shape[1] - 1"]},{"cell_type":"markdown","metadata":{"id":"V1FN1Z-tOFOo"},"source":["> You can add your own global attributes here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQ-OYop8ONnv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Gey7t_Yx5YML"},"source":["## Step2 : Calculate the Entropy and Information Gain \n","Calculate the information gain and entropy values before separate data into left subtree and right subtree"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"hpdNz3ij6keH"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_entropy =  0.98714\n"]}],"source":["from math import log2\n","\n","def entropy(data):\n","  \"\"\"\n","  This function measures the amount of uncertainty in a probability distribution\n","  args: \n","  * data(type: DataFrame): the data you're calculating for the entropy\n","  return:\n","  * entropy_value(type: float): the data's entropy\n","  \"\"\"\n","\n","  total = data.shape[0]\n","  pos_case = data['diabetes_mellitus'].sum()\n","  \n","  if pos_case == 0:\n","    e = 0\n","  elif pos_case == total:\n","    e = 0\n","  else:\n","    p = pos_case/total\n","    e = -p*log2(p)-(1-p)*log2(1-p)\n","\n","  entropy_value = round(e,5)\n","  \n","  return entropy_value\n","\n","# [Note] You have to save the value of \"ans_entropy\" into the output file\n","ans_entropy = entropy(input_data)\n","print(\"ans_entropy = \", ans_entropy)"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"zCC_SiU26kbX"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_informationGain =  0.08346\n"]}],"source":["def information_gain(data, mask):\n","  \"\"\"\n","  This function will calculate the information gain\n","  args:\n","  * data(type: DataFrame): the data you're calculating for the information gain\n","  * mask(type: Series): partition information(left/right) of current input data, \n","    - boolean 1(True) represents split to left subtree\n","    - boolean 0(False) represents split to right subtree\n","  return:\n","  * ig(type: float): the information gain you can obtain by classify data with this given mask\n","  \"\"\"\n","  #Split data base on mask\n","  left_tree_true = data[mask]\n","  right_tree_false = data[~mask]\n","\n","  #Calculate the number of cases in left and right subtree respectively\n","  n_left_tree = left_tree_true.shape[0]\n","  n_right_tree = right_tree_false.shape[0]\n","  n_total = data.shape[0]\n","\n","  #Calculate the weight of left and right subtree \n","  weight_left = n_left_tree/n_total\n","  weight_right = n_right_tree/n_total\n","\n","  #Claculate the entropy of left and right subtrees\n","  left_entropy = entropy(left_tree_true)\n","  right_entropy = entropy(right_tree_false)\n","\n","  entropy_before = entropy(data)\n","  entropy_after = weight_left * left_entropy + weight_right * right_entropy\n","  \n","  ig = round(entropy_before - entropy_after,5)\n","\n","  return ig\n","\n","# [Note] You have to save the value of \"ans_informationGain\" into your output file\n","temp1 = np.zeros((int(input_data.shape[0]/4), 1), dtype=bool)\n","temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/4), 1)), dtype=bool)\n","temp_mask = np.concatenate((temp1, temp2))\n","df_mask = pd.DataFrame(temp_mask, columns=['mask'])\n","ans_informationGain = information_gain(input_data, df_mask['mask'])\n","print(\"ans_informationGain = \", ans_informationGain)"]},{"cell_type":"markdown","metadata":{"id":"9r8mrn7A55if"},"source":["## Step3 : Find the Best Split\n","Find the best split combination, **feature** and **threshold**, by calculating the information gain\n"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"D6gg7ig18XgM"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_ig =  0.35229\n","ans_value =  153.0\n","ans_name =  glucose_apache\n"]}],"source":["def find_best_split(data):\n","  \"\"\"\n","  This function will find the best split combination of data\n","  args:\n","  * data(type: DataFrame): the input data\n","  return\n","  * best_ig(type: float): the best information gain you obtain\n","  * best_threshold(type: float): the value that splits data into 2 branches\n","  * best_feature(type: string): the feature that splits data into 2 branches\n","  \"\"\"\n","  df = data\n","  features = df.columns\n","  n = n_features\n","  n_case = int(data.shape[0])\n","\n","  step = 1\n","\n","  table = np.zeros((3, n), dtype=float)\n","\n","  mask_frag = np.ones((step, 1), dtype=bool)\n","\n","  for i in range(n):\n","    mask = np.zeros((n_case, 1), dtype=bool)\n","    df['mask'] = mask\n","    for j in range(0,n_case-1,step):\n","      select = df.sort_values(by=[features[i]])\n","      mask[j:j+step] = mask_frag\n","      a = select[features[i]][j+step-1]\n","      b = select[features[i]][j+step]\n","      if a == b:\n","        continue\n","      select['mask'] = mask\n","      t_threshold = (select[features[i]][j+step]+select[features[i]][j+step-1])/2\n","      select = select.sort_index()\n","      t_ig = information_gain(select, select['mask'])\n","      if table[0][i] < t_ig:\n","        table[0][i] = t_ig\n","        table[1][i] = t_threshold\n","        table[2][i] = i\n","  \n","  temp = pd.DataFrame(table)\n","  result = temp.sort_values(by=[0], ascending=False, axis=1).columns\n","  index = int(result[0])\n","  best_ig = temp[index][0]\n","  best_threshold = temp[index][1]\n","  best_feature = features[int(temp[index][2])]\n","\n","  return best_ig, best_threshold, best_feature\n","\n","\n","# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n","ans_ig, ans_value, ans_name = find_best_split(input_data)\n","print(\"ans_ig = \", ans_ig)\n","print(\"ans_value = \", ans_value)\n","print(\"ans_name = \", ans_name)"]},{"cell_type":"markdown","metadata":{"id":"61hPUYvy6MTB"},"source":["## Step4 : Split into 2 branches\n","Using the best split combination you find in function *find_best_split()* to split data into Left Subtree and Right Subtree "]},{"cell_type":"code","execution_count":55,"metadata":{"id":"KQRcjzCLCo4R"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_left =  10\n"]}],"source":["def make_partition(data, feature, threshold):\n","  \"\"\"\n","  This function will split the data into 2 branches\n","  args:\n","  * data(type: DataFrame): the input data\n","  * feature(type: string): the attribute(column name)\n","  * threshold(type: float): the threshold for splitting the data\n","  return:\n","  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n","  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n","  \"\"\"\n","  n = int(data.shape[0])\n","  mask = np.zeros((n, 1), dtype=bool)\n","  for i in range(n):\n","    mask[i] = (data[feature][i] <= threshold)\n","\n","  left = data[mask]\n","  right = data[~mask]\n","\n","  return left, right\n","\n","\n","# [Note] You have to save the value of \"ans_left\" into the output file\n","left, right = make_partition(input_data, 'age', 61.0)\n","ans_left = left.shape[0]\n","print(\"ans_left = \", ans_left)"]},{"cell_type":"markdown","metadata":{"id":"GLzy6Yhg802x"},"source":["## Step5 : Build Decision Tree\n","Use the above functions to implement the decision tree\n","\n","Instructions: \n","1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n","2.  Use function *find_best_split()* to find the best split combination\n","3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n","4. Use function *make_partition()* to split the data into two parts\n","5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively\n","\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"_OAXVddKkvM2"},"outputs":[],"source":["def build_tree(data, max_depth, min_samples_split, depth):\n","  \"\"\"\n","  This function will build the decision tree\n","  args:\n","  * data(type: DataFrame): the data you want to apply to the decision tree\n","  * max_depth: the maximum depth of a decision tree\n","  * min_samples_split: the minimum number of instances required to do partition\n","  * depth: the height of the current decision tree\n","  return:\n","  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n","  \"\"\"\n","\n","  # check the condition of current depth and the remaining number of samples\n","  if depth < max_depth:\n","    # call find_best_split() to find the best combination\n","    ig, threshold, feature = find_best_split(data)\n","    # check the value of information gain is greater than 0 or not \n","    if ig > 0:\n","      # update the depth\n","      depth = depth + 1\n","      # call make_partition() to split the data into two parts\n","      left_st, right_st = make_partition(data, feature, threshold)\n","      # If there is no data split to the left tree OR no data split to the left tree\n","      if left_st.empty:\n","        # return the label of the majority\n","        t = right_st.shape[0]\n","        p = right_st['diabetes_mellitus'].sum()\n","        n = t-p\n","        label = int(p > n)\n","        return label\n","      elif right_st.empty: \n","        t = left_st.shape[0]\n","        p = left_st['diabetes_mellitus'].sum()\n","        n = t-p\n","        label = int(p > n)\n","        return label\n","      else:\n","        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n","        subtree = {question: []}\n","        # call function build_tree() to recursively build the left subtree and right subtree\n","        left_subtree = build_tree(left_st, max_depth, min_samples_split, depth)\n","        right_subtree = build_tree(right_st, max_depth, min_samples_split, depth)\n","        if left_subtree == right_subtree:\n","          subtree = left_subtree\n","        else:\n","          subtree[question].append(left_subtree)\n","          subtree[question].append(right_subtree)\n","    else:\n","      # return the label of the majority\n","      t = data.shape[0]\n","      p = data['diabetes_mellitus'].sum()\n","      n = t-p\n","      label = int(p > n)\n","      return label\n","  else:\n","    # return the label of the majority\n","    t = data.shape[0]\n","    p = data['diabetes_mellitus'].sum()\n","    n = t-p\n","    label = int(p > n)\n","    return label\n","\n","  return subtree"]},{"cell_type":"markdown","metadata":{"id":"qlIrw9Gu-M9-"},"source":["An example of the output from *build_tree()* \n","```\n","{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n","```\n","Therefore, \n","```\n","ans_features = ['bmi', 'age']\n","ans_thresholds = [33.5, 68.5]\n","```\n","\n"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"QW8wm1rD9dlS"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\KCChu\\AppData\\Local\\Temp\\ipykernel_14548\\333235832.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['mask'] = mask\n"]},{"ename":"KeyError","evalue":"4","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 4","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\Users\\KCChu\\Documents\\GitHub\\ML-Diabetes-Prediction\\hw2.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ans_features \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ans_thresholds \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m decisionTree \u001b[39m=\u001b[39m build_tree(input_data, max_depth, min_samples_split, depth)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m decisionTree\n","\u001b[1;32mc:\\Users\\KCChu\\Documents\\GitHub\\ML-Diabetes-Prediction\\hw2.ipynb Cell 21\u001b[0m in \u001b[0;36mbuild_tree\u001b[1;34m(data, max_depth, min_samples_split, depth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m subtree \u001b[39m=\u001b[39m {question: []}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# call function build_tree() to recursively build the left subtree and right subtree\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m left_subtree \u001b[39m=\u001b[39m build_tree(left_st, max_depth, min_samples_split, depth)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m right_subtree \u001b[39m=\u001b[39m build_tree(right_st, max_depth, min_samples_split, depth)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m left_subtree \u001b[39m==\u001b[39m right_subtree:\n","\u001b[1;32mc:\\Users\\KCChu\\Documents\\GitHub\\ML-Diabetes-Prediction\\hw2.ipynb Cell 21\u001b[0m in \u001b[0;36mbuild_tree\u001b[1;34m(data, max_depth, min_samples_split, depth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# check the condition of current depth and the remaining number of samples\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m depth \u001b[39m<\u001b[39m max_depth:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   \u001b[39m# call find_best_split() to find the best combination\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   ig, threshold, feature \u001b[39m=\u001b[39m find_best_split(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   \u001b[39m# check the value of information gain is greater than 0 or not \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   \u001b[39mif\u001b[39;00m ig \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# update the depth\u001b[39;00m\n","\u001b[1;32mc:\\Users\\KCChu\\Documents\\GitHub\\ML-Diabetes-Prediction\\hw2.ipynb Cell 21\u001b[0m in \u001b[0;36mfind_best_split\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m mask[j:j\u001b[39m+\u001b[39mstep] \u001b[39m=\u001b[39m mask_frag\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m a \u001b[39m=\u001b[39m select[features[i]][j\u001b[39m+\u001b[39mstep\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m b \u001b[39m=\u001b[39m select[features[i]][j\u001b[39m+\u001b[39;49mstep]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m a \u001b[39m==\u001b[39m b:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KCChu/Documents/GitHub/ML-Diabetes-Prediction/hw2.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n","File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 4"]}],"source":["ans_features = []\n","ans_thresholds = []\n","\n","decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n","decisionTree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_n0BfNSGejN"},"outputs":[],"source":["# [Note] You have to save the features in the \"decisionTree\" structure (from root to branch and leaf) into the output file\n","ans_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6H9zkN_GgK-"},"outputs":[],"source":["# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n","ans_thresholds"]},{"cell_type":"markdown","metadata":{"id":"rP0SU7tTweOX"},"source":["## Step6 : Save answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDO36kKEwh6C"},"outputs":[],"source":["basic = []\n","basic.append(ans_entropy)\n","basic.append(ans_informationGain)\n","basic.append(ans_ig)\n","basic.append(ans_value)\n","basic.append(ans_name)\n","basic.append(ans_left)\n","for i in range(len(ans_features)):\n","  basic.append(ans_features[i])\n","for m in range(len(ans_thresholds)):\n","  basic.append(ans_thresholds[m])"]},{"cell_type":"markdown","metadata":{"id":"7DotyrSZjYKi"},"source":["## Step7 : Split data\n","Split data into training set and validation set\n","> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjNM-n4i5mlG"},"outputs":[{"name":"stdout","output_type":"stream","text":["(30, 10)\n","(20, 10)\n","(10, 10)\n"]}],"source":["num_train = 20\n","num_validation = 10\n","\n","training_data = input_data.iloc[:num_train]\n","validation_data = input_data.iloc[-num_validation:]\n","\n","y_train = training_data[[\"diabetes_mellitus\"]]\n","x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = validation_data[[\"diabetes_mellitus\"]]\n","x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = y_validation.values.flatten()\n","\n","print(input_data.shape)\n","print(training_data.shape)\n","print(validation_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"GfKSt2gH74Uu"},"source":["## Step8 to Step10 : Make predictions with a decision tree"]},{"cell_type":"markdown","metadata":{"id":"BZqSVoJ48a3-"},"source":["Define the attributions of the decision tree\n","> You **cannot** modify the values of these attributes in this part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSlZ7FVB8eau"},"outputs":[],"source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = x_train.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"FrK-YqLmLH8p"},"source":["We have finished the function '*classify_data()*' below, however, you can modify this function if you prefer completing it on your own way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0piZ0blpFXVq"},"outputs":[],"source":["def classify_data(instance, tree):\n","  \"\"\"\n","  This function will predict/classify the input instance\n","  args:\n","  * instance: a instance(case) to be predicted\n","  return:\n","  * answer: the prediction result (the classification result)\n","  \"\"\"\n","  equation = list(tree.keys())[0] \n","  if equation.split()[1] == '<=':\n","    temp_feature = equation.split()[0]\n","    temp_threshold = equation.split()[2]\n","    if instance[temp_feature] > float(temp_threshold):\n","      answer = tree[equation][1]\n","    else:\n","      answer = tree[equation][0]\n","  else:\n","    if instance[equation.split()[0]] in (equation.split()[2]):\n","      answer = tree[equation][0]\n","    else:\n","      answer = tree[equation][1]\n","\n","  if not isinstance(answer, dict):\n","    return answer\n","  else:\n","    return classify_data(instance, answer)\n","\n","\n","def make_prediction(tree, data):\n","  \"\"\"\n","  This function will use your pre-trained decision tree to predict the labels of all instances in data\n","  args:\n","  * tree: the decision tree\n","  * data: the data to predict\n","  return:\n","  * y_prediction: the predictions\n","  \"\"\"\n","  \n","  # [Note] You can call the function classify_data() to predict the label of each instance\n","\n","\n","  return y_prediction\n","\n","\n","def calculate_score(y_true, y_pred):\n","  \"\"\"\n","  This function will calculate the f1-score of the predictions\n","  args:\n","  * y_true: the ground truth\n","  * y_pred: the predictions\n","  return:\n","  * score: the f1-score\n","  \"\"\"\n","\n","  \n","  return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IEu3z3s9TDu"},"outputs":[],"source":["decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n","\n","y_pred = make_prediction(decision_tree, x_validation)\n","\n","# [Note] You have to save the value of \"ans_f1score\" the your output file\n","ans_f1score = calculate_score(y_validation, y_pred)\n","print(\"ans_f1score = \", ans_f1score)"]},{"cell_type":"markdown","metadata":{"id":"IzzOKOwn-kod"},"source":["## Step11 : Write the Output File\n","Save all of your answers in a csv file, named as **hw2_basic.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0zsaWPL2qXn"},"outputs":[],"source":["ans_path = 'hw2_basic.csv'\n","\n","# [Note] You have to save the value of \"ans_f1score\" into the output file\n","basic.append(ans_f1score)\n","print(basic)\n","\n","pd.DataFrame(basic).to_csv(ans_path, header = None, index = None)"]},{"cell_type":"markdown","metadata":{"id":"tV25IjM7_aEn"},"source":["# **Advanced Part** (35%)"]},{"cell_type":"markdown","metadata":{"id":"knH1Ih0Pha7X"},"source":["## Step1: Load the input data\n","First, load the input file **hw2_input_advanced.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FthBdLxRhi9W"},"outputs":[],"source":["advanced_data = pd.read_csv('hw2_input_advanced.csv')"]},{"cell_type":"markdown","metadata":{"id":"vqLH49oBndRh"},"source":["You can split *advanced_data* into training set and validaiton set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9l0hLPVjncam"},"outputs":[],"source":["training_data = \n","validation_data = "]},{"cell_type":"markdown","metadata":{"id":"tFgbUY_ajVOK"},"source":["## Step2 : Load the test data\n","Load the input file **hw2_input_test.csv** to make predictions with the pre-trained random forest model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hW542KWNxVF"},"outputs":[],"source":["x_test = pd.read_csv('hw2_input_test.csv')\n","x_test"]},{"cell_type":"markdown","metadata":{"id":"mH-0DxyR9qWn"},"source":["## Step3 : Build a Random Forest"]},{"cell_type":"markdown","metadata":{"id":"8xbLxFW597FG"},"source":["Define the attributions of the random forest\n","> * You **can** modify the values of these attributes in advanced part\n","> * Each tree can have different attribute values\n","> * There must be **at least** 3 decision trees in the random forest model\n","> * Must use function *build_tree()* to build a random forest model\n","> * These are the parameters you can adjust : \n","\n","\n","    ```\n","    max_depth = \n","    depth = 0\n","    min_samples_split = \n","    \n","    # total number of trees in a random forest\n","    n_trees = \n","\n","    # number of features to train a decision tree\n","    n_features = \n","\n","    # the ratio to select the number of instances\n","    sample_size = \n","    n_samples = int(training_data.shape[0] * sample_size)\n","    ```\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LD8ndJ8ymzG3"},"outputs":[],"source":["# Define the attributes\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVl66f1aU36-"},"outputs":[],"source":["def build_forest(data, n_trees, n_features, n_samples):\n","  \"\"\"\n","  This function will build a random forest.\n","  args:\n","  * data: all data that can be used to train a random forest\n","  * n_trees: total number of tree\n","  * n_features: number of features\n","  * n_samples: number of instances\n","  return:\n","  * forest: a random forest with 'n_trees' of decision tree\n","  \"\"\"\n","\n","  # must reuse function build_tree()\n","  tree = build_tree(.....)\n","\n","  return forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zylo6C51m3OJ"},"outputs":[],"source":["forest = build_forest(training_data, n_trees, n_features, n_samples)"]},{"cell_type":"markdown","metadata":{"id":"dZb6EEYnnO05"},"source":["## Step4 : Make predictions with the random forest\n","> Note: Please print the f1-score of the predictions of each decision tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbHMZnMDnWpG"},"outputs":[],"source":["def make_prediction_forest(forest, data):\n","  \"\"\"\n","  This function will use the pre-trained random forest to make the predictions\n","  args:\n","  * forest: the random forest\n","  * data: the data used to predict\n","  return:\n","  * y_prediction: the predicted results\n","  \"\"\"\n","\n","\n","  return y_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hcd70ubwgHq4"},"outputs":[],"source":["y_pred_test = make_prediction_forest(forest, x_test)"]},{"cell_type":"markdown","metadata":{"id":"2ufa5bP9HveO"},"source":["## Step5 : Write the Output File\n","Save your predictions from the **random forest** in a csv file, named as **hw2_advanced.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdAQcE41JJYB"},"outputs":[],"source":["advanced = []\n","for i in range(len(y_pred_test)):\n","  advanced.append(y_pred_test[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pq121klSHwWO"},"outputs":[],"source":["advanced_path = 'hw2_advanced.csv'\n","pd.DataFrame(advanced).to_csv(advanced_path, header = None, index = None)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"}}},"nbformat":4,"nbformat_minor":0}
